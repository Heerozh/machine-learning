{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习工程师纳米学位\n",
    "## 深度学习\n",
    "## 项目：搭建一个数字识别项目\n",
    "\n",
    "在此文件中，我们提供给你了一个模板，以便于你根据项目的要求一步步实现要求的功能，进而完成整个项目。如果你认为需要导入另外的一些代码，请确保你正确导入了他们，并且包含在你的提交文件中。以**'练习'**开始的标题表示接下来你将开始实现你的项目。注意有一些练习是可选的，并且用**'可选'**标记出来了。\n",
    "\n",
    "在此文件中，有些示例代码已经提供给你，但你还需要实现更多的功能让项目成功运行。除非有明确要求，你无须修改任何已给出的代码。以'练习'开始的标题表示接下来的代码部分中有你必须要实现的功能。每一部分都会有详细的指导，需要实现的部分也会在注释中以'TODO'标出。请仔细阅读所有的提示！\n",
    "\n",
    "除了实现代码外，你还必须回答一些与项目和你的实现有关的问题。每一个需要你回答的问题都会以**'问题 X'**为标题。请仔细阅读每个问题，并且在问题后的**'回答'**文字框中写出完整的答案。我们将根据你对问题的回答和撰写代码所实现的功能来对你提交的项目进行评分。\n",
    "\n",
    ">**注意：** Code 和 Markdown 区域可通过 **Shift + Enter** 快捷键运行。此外，Markdown可以通过双击进入编辑模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 步骤 1: 设计并测试一个模型架构\n",
    "设计并实现一个能够识别数字序列的深度学习模型。你可以通过连接[notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html)或者是[MNIST](http://yann.lecun.com/exdb/mnist/)的字符来合成数据来训练这个模型。为了产生用于测试的合成数字序列，你可以进行如下的设置：比如，你可以限制一个数据序列最多五个数字，并在你的深度网络上使用五个分类器。同时，你有必要准备一个额外的“空白”的字符，以处理相对较短的数字序列。\n",
    "\n",
    "在思考这个问题的时候有很多方面可以考虑：\n",
    "- 你的模型可以基于深度神经网络或者是卷积神经网络。\n",
    "- 你可以尝试是否在softmax分类器间共享权值。\n",
    "- 你还可以在深度神经网络中使用循环网络来替换其中的分类层，并且将数字序列里的数字一个一个地输出。\n",
    "\n",
    "你可以使用 Keras 来构建你的模型，[这里](https://keras.io/)了解更多。\n",
    "\n",
    "这里有一个[发表的关于这个问题的基准模型的论文](https://github.com/nd009/machine-learning/blob/zh-cn/projects/digit_recognition/42241.pdf) [（视频）](http://cn-static.udacity.com/mlnd/videos/ICLR14_%20Goodfellow.mp4)的例子。我们并不要求你构建的模型架构跟论文上一摸一样或者达到它那样的模型表现。这里紧紧是展示一个解决这个问题的例子。我们鼓励你采用不同的架构，来看看哪种表现对你来说最好。你也可以看论坛上关于论文中架构的[讨论](https://discussions.udacity.com/t/goodfellow-et-al-2013-architecture/202363)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现\n",
    "使用下面的代码单元（必要的话可以使用多个代码单元）来实现你项目的第一步。一旦你完成了你的实现并且获得了满意的结果，请确认全面回答下面相关的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "### 在这里实现你的代码  \n",
    "### 必要的话你可以随意添加更多的代码单元 \n",
    "from __future__ import print_function\n",
    "from imp import reload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "import random\n",
    "import os\n",
    "\n",
    "import mnist_seq\n",
    "mnist_seq = reload(mnist_seq)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "num_seqlen = 5\n",
    "num_labels = mnist_seq.DIGITS\n",
    "\n",
    "# Generate training data\n",
    "random.seed(1)\n",
    "train_data, train_labs = mnist_seq.gen_data(10000, seqs=num_seqlen)\n",
    "vail_data, vail_labs = mnist_seq.gen_data(1000, test=True, seqs=num_seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def argmaxlab(lab):\n",
    "    return np.argmax(lab.reshape(-1, num_seqlen, num_labels), 2)\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(argmaxlab(predictions) == argmaxlab(labels))\n",
    "          / (predictions.shape[0] * num_seqlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", dtype=float32)\n",
      "Tensor(\"predict/conv1/MaxPool:0\", shape=(?, 14, 70, 32), dtype=float32)\n",
      "Tensor(\"predict/conv2/MaxPool:0\", shape=(?, 7, 35, 64), dtype=float32)\n",
      "Tensor(\"predict/conv3/MaxPool:0\", shape=(?, 4, 18, 64), dtype=float32)\n",
      "Tensor(\"predict/fully_connected/dropout/mul:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"predict/out/add:0\", shape=(?, 55), dtype=float32)\n",
      "Tensor(\"predict/conv1_1/MaxPool:0\", shape=(1000, 14, 70, 32), dtype=float32)\n",
      "Tensor(\"predict/conv2_1/MaxPool:0\", shape=(1000, 7, 35, 64), dtype=float32)\n",
      "Tensor(\"predict/conv3_1/MaxPool:0\", shape=(1000, 4, 18, 64), dtype=float32)\n",
      "Tensor(\"predict/fully_connected_1/Relu:0\", shape=(1000, 256), dtype=float32)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.829253\n",
      "Minibatch accuracy: 12.0%\n",
      "Test accuracy: 11.1%\n",
      "Minibatch loss at step 50: 0.217758\n",
      "Minibatch accuracy: 43.9%\n",
      "Test accuracy: 46.4%\n",
      "Minibatch loss at step 100: 0.121950\n",
      "Minibatch accuracy: 75.0%\n",
      "Test accuracy: 80.8%\n",
      "Minibatch loss at step 150: 0.071584\n",
      "Minibatch accuracy: 89.1%\n",
      "Test accuracy: 93.2%\n",
      "Minibatch loss at step 200: 0.051692\n",
      "Minibatch accuracy: 92.7%\n",
      "Test accuracy: 95.4%\n",
      "Minibatch loss at step 250: 0.040830\n",
      "Minibatch accuracy: 94.5%\n",
      "Test accuracy: 96.3%\n",
      "Minibatch loss at step 300: 0.035253\n",
      "Minibatch accuracy: 96.2%\n",
      "Test accuracy: 96.7%\n",
      "Minibatch loss at step 350: 0.037684\n",
      "Minibatch accuracy: 96.1%\n",
      "Test accuracy: 96.7%\n",
      "Minibatch loss at step 400: 0.028014\n",
      "Minibatch accuracy: 96.6%\n",
      "Test accuracy: 97.4%\n",
      "Test accuracy: 97.4%\n"
     ]
    }
   ],
   "source": [
    "#fit model\n",
    "import helper\n",
    "helper = reload(helper)\n",
    "\n",
    "def my_loss_func(logits, tf_train_labs):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits, tf_train_labs))\n",
    "\n",
    "# Main Model\n",
    "def my_model(x_input, drop=None):\n",
    "    depth1, depth2, depth3, depth4 = 32, 64, 64, 256\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        relu = helper.conv_relu(x_input, kernel_shape=[3, 3, 1, depth1],  pool=True)\n",
    "        \n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        relu = helper.conv_relu(relu, kernel_shape=[3, 3, depth1, depth2],  pool=True)\n",
    "        \n",
    "    with tf.variable_scope(\"conv3\"):\n",
    "        relu = helper.conv_relu(relu, kernel_shape=[3, 3, depth2, depth3], pool=True)\n",
    "        \n",
    "    with tf.variable_scope(\"fully_connected\"):\n",
    "        shape = relu.get_shape().as_list()\n",
    "        csize = shape[1] * shape[2] * shape[3]\n",
    "        reshape = tf.reshape(relu, [-1, csize])    \n",
    "        relu = helper.relu(reshape, kernel_shape=[csize, depth4], drop=drop)\n",
    "        \n",
    "    with tf.variable_scope(\"out\"):\n",
    "        weights, biases = helper.var([depth4, num_labels * num_seqlen]) #11 * 5 = 55\n",
    "        logits = tf.matmul(relu, weights) + biases\n",
    "        return logits, tf.nn.sigmoid(logits)\n",
    "\n",
    "clf = helper.Learner(my_model, accuracy, \n",
    "                     steps=401, batch_size=128, learning_rate=0.01,\n",
    "                     loss=tf.nn.sigmoid_cross_entropy_with_logits,\n",
    "                     optimizer=my_loss_func\n",
    "                     )\n",
    "\n",
    "clf.fit(train_data, train_labs, vail_data, vail_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "False 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\__main__.py:11: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABzCAYAAACIEflfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEWpJREFUeJzt3XmUVOWZx/HvQ7MrIG20wxZAQZTxqLiiKDHiRGRUkjmO\nATdMOAedwXGJSdRxEs1knHhMjpqMGENc0AlRiYrgLjJ6mGRAAXUIiyCILMoiB3ej0PQzf9xbt6q7\nq7qrumu7l9/nnD7c+7636j5NVb391ruauyMiIvHXodIBiIhIcahAFxFJCBXoIiIJoQJdRCQhVKCL\niCSECnQRkYRQgS4ikhDtKtDNbKyZrTaztWZ2XbGCEhGRwllbJxaZWQ2wBvhbYDOwGJjo7iuLF56I\niOSrYzseezyw1t3fBjCzh4HxQM4CvbN18a7s045biojsfT7hgx3ufkBr17WnQO8HbMo43wyc0PQi\nM5sCTAHoSndOsDHtuKWIyN7nRX90Qz7XlbxT1N2nu/ux7n5sJ7qU+nYiInut9hTo7wIDMs77h2ki\nIlIB7SnQFwNDzWywmXUGJgBzixOWiIgUqs1t6O5eb2aXA88DNcB97r6iaJGJiEhB2tMpirs/AzxT\npFhERKQdNFNURCQhVKCLiCSECnQRkYRoVxt63K29fSQA675zd5R28YbRAGw78eOKxFRKDaeMAODH\nD9wPwKguDVHeXR8OBuDBX46L0mrvX1jG6Crj828Hc+EG/2hVlPbgwAWtPi71PoHkvVd2XHpidLzo\nJ3cCMGbqP0Zp3Z54tewxSX5UQxcRSYi9roaeqpFB45p50nx55nEAbDg3vfjaI6f9BoARnYO/4w0Z\n11+239sAjP/pL6K0M8YFtbKB568GwHfvKlm85Zbt21khMmvxFy9M1re62pVfRMfr64PjTWPT+Yc8\nUe6IJF+qoYuIJMReV0PvPvuV9Mm05vl/XjQcgCEsKlNExdWxfz8Ajrh5CQDz6pZEeTM/CfIufvBc\nADrUpx93xcQ5AEzutTFKW3bSDABOeOx8AOombo7yGj77rMiRl16qVg7F/XaWqq2fwVFFe85K2nlY\n1+h4YMfOAPxqzO+jtGkcUvaYJD+qoYuIJIQKdBGRhNjrmlwyO0XhjWb5fRe0bQenarHjG18D4D/q\ngnXSOmBR3qxzTgFg4Jr/bfa4J393OAB3/Pq0KG3JSb8D4JVj/gDAsTMvjPL6TlgPQMMX6Q60apV6\nzfNtZjll6qVAk+a5UN3CnkD2oY2ZTTpDro5nk53Em2roIiIJsdfV0N8bbS3mZ6uVxcn2U3c3Op//\n14xNRba+n/Nx9Vu3ATDwvG1R2jE3Xw3A8kuCySWvHTczyjvi6ssB6P/z5rX9apM5aSiXVK0cWn4P\npDrNyWPyUVzt7pH+jHQI63w11pDrcqkiqqGLiCSECnQRkYRotcnFzO4DzgK2u/vhYVot8AgwCHgH\nOM/dPyhdmO3XUsdY5rocEL/Zfh0HfS06vvbEZwH40oOml5v+dWqU1+PjwjrqBv04WLPj0G7Bc6z5\nzl1R3kv/FMwo/fa6a6K0fWfFryMw9drn29Q2auTKUoZTFUZPWBodN4Tzia9YNDFKG8LrZY9J8pNP\nDX0GMLZJ2nXAfHcfCswPz0VEpIJaraG7+wIzG9QkeTxwanj8APAycG0R4yq6qAaWZXZo5hC0OM72\nW3NZv+h4cq/ZAMz97EAAejzcjlpzwx4ADr1tEwCLvpXOOr5LMJvw/aPTHWj7zmr7rUpp/a2HAXDw\n6OFRWmp4ar4189SQxOcHJnf9Hx8VvPevODDzQxLMFO20rlsFIpJCtXWUS527bwmPtwJ1uS40synA\nFICudG/j7UREpDXtHrbo7m5mOWfjuPt0YDpAT6ut2Kyd1ISQbBoNWSN+wxbre+4p7fNvfheA79+Y\nbo//0y3BUMb6HtU/nC1VCx8yu7DH5bsyZ6odPu6TiXb17ASk12+R+GnrKJdtZtYHIPx3e/FCEhGR\ntmhrgT4XmBQeTwLmFCccERFpq3yGLT5E0AH6FTPbDNwI3ALMMrPJwAbgvFIGWQwtbSuWOXu00K/l\n1WDYoe82S/vhwmCJ3KG8VrT77L9wa7O0m8Y8Fh3PpH/R7lVJ6fVafpvzmiRvQdchWz0vd6uqVJF8\nRrlMzJE1psixiIhIOyR+LZeWOkNTtay4r7B4ft/mHbn95nQq+n22nfbVZmm3LE9PURjA8qLfs9RS\nHZ//My13bTxTkjcR33hmamvC5h3dg+akf994f1qSTVP/RUQSIvE19HxWx4vrCou7Tz8GgHH7/Dkj\nNZjw0/P1oL27vumD2uGsqc3/D+vX9ijiHconn3byTAc/chkQ/6GJ2Xxx1vEAPHz2nc3y7vnoIAB8\n6YqyxiRtoxq6iEhCqEAXEUmIRDa5tDTDL3O4WWqNjzjODgXY3TN4+Xp3SK+z8e87gq3kGnbsLNp9\nOn41WNlhTI/ni/aclfD8e823HMwm9R5JvT8AhsxOXlNLysbxQTfnkVkmiN4z7WwADqT6NzIR1dBF\nRBIjkTX0Rp2cTVZXbLSy4uxkDD1ryBhI1qvj5wBYzQHtft4O3YPF1A58InjOE7uk14z5zw+GAjDk\nnvRko9KuKFO41De1fLagy/rNLaad5W2VbUJRn3nBloTV9tpKdqqhi4gkhAp0EZGESGSTS2ozgkDj\njrC4bzfXmqn7rQNgXu8jg4QPPyro8akOUIA1VwVjkOcOCMYnf9TwRZT31PdPA6DT2iVtjrUUMmcG\n5zPGPNv48rh2kheiZsjg6Pj6k58G0jNEr37vlCjPt2gh1ThRDV1EJCESWUPfW3i4SGQHrOUL81Cz\nXy8A3rryoCjtzYuCHuXUyh6n3/rDKK/uheoaxpae+Zl7RnC2js8kD0dsydsX9YmOv9sz2GIw9Tov\n3pbedLz2kzXlDEvaSTV0EZGESFQNPTVMbdTIlTmvyZwsEve20tQS1Q1Z1r/7fFiwSXTn9Rvyeq5V\nPx8GwJpz0ut5fOq7ABjx4uUADJvxlyivGjaey+wraWnz5mwrJLb1tU/dM9t7LE4rME499+mceR8t\n2z86ri1HMFI0rdbQzWyAmb1kZivNbIWZXRmm15rZPDN7K/y3d+nDFRGRXPJpcqkHrnH34cBIYKqZ\nDQeuA+a7+1BgfnguIiIVks+ORVuALeHxJ2a2CugHjCfYmg7gAeBl4NqSRJmn1My+B6flXrMjSbP/\nerz5AQDLdqXn8R3VOXhJu10bbEtnqwdGeQ09gjVf3vn74Iv0yWf+X5T3WL9fA9CB9IIe45ZfAMDQ\nS5YGjy9u+GVT6Jo9jYe9BtJrAjV/bzUeChsPB3XZFh13shoAduwJhqV+bd6XFYlJ2q+gNnQzGwSM\nAF4B6sLCHmArUJfjMVOAKQBd6d7WOEVEpBV5F+hmti/wGHCVu39slh4q5+5uln0XWXefDkwH6Gm1\nJd29Kp/t5pI0mWjPymBI2c82nh2l/XHIMwDMOeRJAF6dn36djuwcdHJ2s6AWntmZ2kBQSxvy9KVR\n2mE/WB3cp+iRl1e0vdy0lq9Ly29VxpQ4dYZ27NcXgB4d0r/jbg9e4Z9sDbYJrnmpeBuLS3nlNWzR\nzDoRFOYz3f3xMHmbmfUJ8/sAmlImIlJB+YxyMeBeYJW735aRNReYFB5PAuYUPzwREclXPk0uo4CL\ngL+YWep72r8AtwCzzGwysAE4rzQh5q+l/UNTeUNI3szAtc8eHB13+OegiSXVnDKyS/q6Bjo1etx3\nN4yJjtffHoxDP+SP6Y7DuDe1lELc9xb97Ih+AAzq+GlGatBZ/vLcowEYoM0sYiufUS5/gpxzy8fk\nSBcRkTJL1EzRlmaI9l1Q0v7Yiup/+9LoeNgR3wNg1dfvBRp3fB7yXNDhOfiR4LzTvPTj9vX4DefM\nrCWfsiD43aIO0DylOsujb3c5nj9Ki/k3vC7PLgbgjS8PjNImbwzqZQPvCGYCx3V4qmgtFxGRxDD3\n8tVce1qtn2BqpRERKcSL/uhSdz+2tetUQxcRSQgV6CIiCaECXUQkIVSgi4gkhAp0EZGEUIEuIpIQ\nKtBFRBJCBbqISEKoQBcRSYiyzhQ1s/eBz4AdZbtp8X0FxV9JcY4/zrGD4q+kge5+QGsXlbVABzCz\nJflMYa1Wir+y4hx/nGMHxR8HanIREUkIFegiIglRiQJ9egXuWUyKv7LiHH+cYwfFX/XK3oYuIiKl\noSYXEZGEUIEuIpIQZS3QzWysma02s7Vmdl05710oMxtgZi+Z2UozW2FmV4bptWY2z8zeCv/tXelY\nW2JmNWb2upk9FZ7HJn4z28/MHjWzN81slZmdGLP4rw7fO8vN7CEz61rN8ZvZfWa23cyWZ6TljNfM\nrg8/y6vN7IzKRJ2WI/5fhO+fZWY228z2y8irqviLoWwFupnVANOAM4HhwEQza74zb/WoB65x9+HA\nSGBqGO91wHx3HwrMD8+r2ZXAqozzOMX/K+A5dz8UOJLg94hF/GbWD7gCONbdDwdqgAlUd/wzgLFN\n0rLGG34WJgB/Ez7mrvAzXkkzaB7/POBwdz8CWANcD1Ubf7uVs4Z+PLDW3d92913Aw8D4Mt6/IO6+\nxd1fC48/IShM+hHE/EB42QPAtyoTYevMrD/wd8A9GcmxiN/MegGjgXsB3H2Xu39ITOIPdQS6mVlH\noDvwHlUcv7svAHY2Sc4V73jgYXf/0t3XA2sJPuMVky1+d3/B3evD00VA//C46uIvhnIW6P2ATRnn\nm8O0qmdmg4ARwCtAnbtvCbO2AnUVCisfdwA/Ahoy0uIS/2DgfeD+sMnoHjPbh5jE7+7vAr8ENgJb\ngI/c/QViEn+GXPHG8fP8PeDZ8DiO8bdKnaKtMLN9gceAq9z948w8D8Z8VuW4TzM7C9ju7ktzXVPN\n8RPUbo8GfuPuIwjWAGrUPFHN8YdtzeMJ/jD1BfYxswszr6nm+LOJW7yZzOwGgmbUmZWOpZTKWaC/\nCwzIOO8fplUtM+tEUJjPdPfHw+RtZtYnzO8DbK9UfK0YBZxjZu8QNG+dZma/Jz7xbwY2u/sr4fmj\nBAV8XOI/HVjv7u+7+27gceAk4hN/Sq54Y/N5NrNLgLOACzw98SY28ReinAX6YmComQ02s84EHRJz\ny3j/gpiZEbTfrnL32zKy5gKTwuNJwJxyx5YPd7/e3fu7+yCC/+v/dvcLiU/8W4FNZjYsTBoDrCQm\n8RM0tYw0s+7he2kMQT9MXOJPyRXvXGCCmXUxs8HAUODVCsTXIjMbS9DseI67f56RFYv4C+buZfsB\nxhH0NK8DbijnvdsQ68kEXy+XAW+EP+OA/Ql6+98CXgRqKx1rHr/LqcBT4XFs4geOApaEr8ETQO+Y\nxf9T4E1gOfBfQJdqjh94iKC9fzfBN6TJLcUL3BB+llcDZ1Zp/GsJ2spTn+G7qzX+Yvxo6r+ISEKo\nU1REJCFUoIuIJIQKdBGRhFCBLiKSECrQRUQSQgW6iEhCqECXsjGzQWb2VzN7I9t5lutvNrNNZvZp\nAfcoaEnUlpZXbeExzZZpzXLNaDN7zczqzezcJnmTwuVo3zKzSRnpM81sZ+p6C5Zv/tTMEr1TvRSP\nCnQpt3XuflQL55mepIAV8Nq4JGrW5VVbMYPmy7Q2tRG4BPhDkxhrgRuBEwh+txtTa4y7+wVkzJ52\n928QTKwSyYsKdKla7r7I0yv95aPgJVE99/KqLT0m2zKzTa95x92X0XilS4AzgHnuvtPdPyD4g9La\nHweRvKhAlyRp75Komcurlkoil22V6qACXYS9Z3lVSTYV6JIkbVoSNcfyqqWSyGVbpTqoQJckKXhJ\n1BaWVy2V54FvmlnvsDP0m2GaSLupQJeqZWa3mtlmoLuZbTazm1q63t1XALMI1k1/Dpjq7ntauc2d\nQA9gnpm9YWZ35xHXQ8BCYFgY1+Qs1xwXxv4PwG/NbEUY407gZwT7AywG/i1ME2k3LZ8rZRPuzfqU\nux+e7VzAzGYQ/J88Gp6/DPzA3TV8UVqlGrqU0x6gV8ZEoqbnezUzmwl8HfgiPH8JOIhgwwaRVqmG\nLiKSEKqhi4gkhAp0EZGEUIEuIpIQKtBFRBLi/wEGLRhDATQncAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28324b13ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test predict\n",
    "sample_idx = np.random.randint(len(vail_data))  # pick a random image index\n",
    "sample_image = vail_data[sample_idx, :]  # extract a 2D slice\n",
    "plt.imshow(sample_image.reshape(28, 28*5))  # display it\n",
    "\n",
    "predict_lab = clf.predict(np.array([sample_image]))\n",
    "\n",
    "plt.xlabel( argmaxlab(predict_lab[0]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 1\n",
    "_你为解决这个问题采取了什么方法？_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答：** 由于是图像识别，所以我考虑了使用卷积+池化来做。困难的是输出方面，我想用单层输出，考虑到单层输出5个的话无法用softmax来计算loss，所以我决定采用sigmoid。最后实验下来效果不错，但不知道能否用到实际数据上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 2\n",
    "_你最终的模型架构是什么样的？（什么类型的模型，层数，大小, 连接性等）_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答：** 我采取了3个卷积层加池化，1个relu全连接层+drop，最后一个输出层, 输出一个5x11的矩阵，用sigmoid计算loss的训练方法来解决这个问题."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 3\n",
    "_你是如何训练你的模型的？你是如何合成数据集的？_请同时包括你创建的合成数据中的一些示例图像。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答：** 使用了128一个批次进行训练，训练了400步。数据合成方式是先rand获得数字长度，然后从所有数据里随机选择数字图片，采用hstack合成一副28*140的图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4   5   6   7   8   9  B\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]] (5, 11)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABlCAYAAABdnhjZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMfH/pBxzIYq7f9cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2832472d668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test display\n",
    "plt.figure()\n",
    "rndsel = random.randint(0, len(train_data)-1)\n",
    "plt.imshow(train_data[rndsel][:, :, -1]) \n",
    "print('   0   1   2   3   4   5   6   7   8   9  B')\n",
    "print(train_labs[rndsel], train_labs[rndsel].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 步骤 2: 在真实数据集上训练一个模型\n",
    "\n",
    "一旦你确定好了一个好的模型架构，你就可以开始在真实的数据上训练你的模型了。特别地，[街景房屋门牌号(SVHN)](http://ufldl.stanford.edu/housenumbers/)数据集是一个大规模的，从谷歌街景数据中采集的门牌号数据。在这个更有挑战性的数据集（这里数字不是整齐排列的，并且会有各种倾斜、字体和颜色）上训练，可能意味着你必须做一些超参数探索以获得一个表现良好的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现\n",
    "使用下面的代码单元（必要的话可以使用多个代码单元）来实现你项目。一旦你完成了你的实现并且获得了满意的结果，请确认全面回答下面相关的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVHN_data/train already present - Skipping extraction of SVHN_data/train.tar.gz.\n",
      "['SVHN_data/train\\\\train']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\scipy\\ndimage\\interpolation.py:568: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "### 在这里实现你的代码 \n",
    "### 必要的话你可以随意添加更多的代码单元 \n",
    "import svhn\n",
    "svhn = reload(svhn)\n",
    "\n",
    "dataset = svhn.read_data_sets(\"SVHN_data/\")\n",
    "\n",
    "train_images, train_labels = dataset['train_images'], dataset['train_labels']\n",
    "validation_images, validation_labels = dataset['validation_images'], dataset['validation_labels']\n",
    "test_images, test_labels = dataset['test_images'], dataset['test_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "rndsel = 0#random.randint(0, len(train_images)-1)\n",
    "plt.imshow(validation_images[rndsel][:, :]) \n",
    "print('   0   1   2   3   4   5   6   7   8   9  B')\n",
    "print(validation_images[rndsel], validation_images[rndsel].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "helper = reload(helper)\n",
    "\n",
    "#fit model\n",
    "def argmaxlab_svhn(lab):\n",
    "    return np.argmax(lab.reshape(-1, num_seqlen, num_labels), 2)\n",
    "\n",
    "def accuracy_svhn(predictions, labels):\n",
    "    return (100.0 * np.sum(argmaxlab_svhn(predictions) == argmaxlab_svhn(labels))\n",
    "          / (predictions.shape[1] * num_seqlen))\n",
    "\n",
    "def my_loss_func_svhn(logits, tf_train_labs):\n",
    "    tf_train_labs_spd = tf.reshape(tf_train_labs, (-1, 5, 11))\n",
    "    def loss_one(i):\n",
    "#         print(logits[i].get_shape().ndims, tf_train_labs_spd[:, i])\n",
    "        return tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits[i], tf_train_labs_spd[:, i]\n",
    "            ))\n",
    "    return loss_one(0) + loss_one(1) + loss_one(2) + loss_one(3) + loss_one(4)\n",
    "    \n",
    "# Main Model\n",
    "def my_model_svhn(x_input, drop=None):\n",
    "    depth1, depth2, depth3, depth4 = 32, 64, 128, 128\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        relu = helper.conv_relu(x_input, kernel_shape=[3, 3, 1, depth1],  pool=True)\n",
    "        \n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        relu = helper.conv_relu(relu, kernel_shape=[3, 3, depth1, depth2],  pool=True)\n",
    "        \n",
    "    with tf.variable_scope(\"conv3\"):\n",
    "        relu = helper.conv_relu(relu, kernel_shape=[3, 3, depth2, depth3], pool=True)\n",
    "        \n",
    "    with tf.variable_scope(\"fully_connected\"):\n",
    "        shape = relu.get_shape().as_list()\n",
    "        csize = shape[1] * shape[2] * shape[3]\n",
    "        reshape = tf.reshape(relu, [-1, csize])    \n",
    "        relu = helper.relu(reshape, kernel_shape=[csize, depth4], drop=drop)\n",
    "        \n",
    "    with tf.variable_scope(\"out1\"):\n",
    "        weights, biases = helper.var([depth4, num_labels]) #11\n",
    "        lg1 = tf.matmul(relu, weights) + biases\n",
    "    with tf.variable_scope(\"out2\"):        \n",
    "        weights, biases = helper.var([depth4, num_labels]) #11 \n",
    "        lg2 = tf.matmul(relu, weights) + biases\n",
    "    with tf.variable_scope(\"out3\"):\n",
    "        weights, biases = helper.var([depth4, num_labels]) #11 \n",
    "        lg3 = tf.matmul(relu, weights) + biases\n",
    "    with tf.variable_scope(\"out4\"):\n",
    "        weights, biases = helper.var([depth4, num_labels]) #11 \n",
    "        lg4 = tf.matmul(relu, weights) + biases\n",
    "    with tf.variable_scope(\"out5\"):\n",
    "        weights, biases = helper.var([depth4, num_labels]) #11 \n",
    "        lg5 = tf.matmul(relu, weights) + biases\n",
    "        loss_pack = [lg1, lg2, lg3, lg4, lg5, ]\n",
    "        return loss_pack, tf.pack([tf.nn.softmax(lg1), tf.nn.softmax(lg2), tf.nn.softmax(lg3),\n",
    "                                  tf.nn.softmax(lg4), tf.nn.softmax(lg5)])\n",
    "\n",
    "clf_svhn = helper.Learner(my_model_svhn, accuracy_svhn, \n",
    "                     steps=401, batch_size=128, learning_rate=0.01,\n",
    "                     loss=my_loss_func_svhn,\n",
    "                     optimizer=tf.train.AdadeltaOptimizer\n",
    "                     )\n",
    "\n",
    "clf_svhn.fit(train_images[:,:,:,None], train_labels, validation_images[:500,:,:,None], validation_labels[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test predict\n",
    "sample_idx = np.random.randint(len(test_images))  # pick a random image index\n",
    "sample_image = test_images[sample_idx, :]  # extract a 2D slice\n",
    "plt.imshow(sample_image.reshape(66, 150))  # display it\n",
    "\n",
    "predict_lab = clf_svhn.predict(np.array([sample_image]))\n",
    "print(predict_lab)\n",
    "plt.xlabel( np.argmax(predict_lab, axis=2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 4\n",
    "_描述如何为模型准备训练和测试数据。 模型在真实数据集上表现怎么样？_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 5\n",
    "_你（在模型上）做了什么改变？如果做了一些改变，那么你得到一个“好的”结果了妈？有没有任何你探索的导致结果更糟？_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 6\n",
    "_当你在真实数据集做测试的时候你的初始结果和最终结果是什么？你认为你的模型在正确分类数字这个任务上上做的足够好吗？_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 步骤 3: 在新抓取的图片上测试模型\n",
    "\n",
    "在你周围拍摄几张数字的图片（至少五张），然后用你的分类器来预测产生结果。或者（可选），你可以尝试使用OpenCV / SimpleCV / Pygame从网络摄像头捕获实时图像，并通过你的分类器分析这些图像。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现\n",
    "使用下面的代码单元（必要的话可以使用多个代码单元）来实现你项目的。一旦你完成了你的实现并且获得了满意的结果，请确认全面回答下面相关的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### 在这里实现你的代码  \n",
    "### 必要的话你可以随意添加更多的代码单元 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 7\n",
    "_选择在你周围拍摄的五张候选图片，并提供在报告中。它们中的某些图片是否有一些特殊的性质，可能会导致分类困难？_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 8\n",
    "_与在现实数据集上的测试结果相比，你的模型能够在捕获的图片或实时相机流上表现同样良好吗？_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可选: 问题 9\n",
    "_如果必要的话，请提供关于你是如何建立一个使得你的模型能够加载和分类新获取图像的接口的。_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答:** 如果你没有完成这个部分，那么这一块请保留空白"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### 步骤 4: 探索一种提升模型的方式\n",
    "\n",
    "一旦你基本的分类器训练好了，你就可以做很多事情。一个例子是：（在分类的同时）还能够定位图像上数字的位置。SVHN数据集提供边界框，你可以调试以训练一个定位器。训练一个关于坐标与边框的回归损失函数，然后测试它。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现\n",
    "使用下面的代码单元（必要的话可以使用多个代码单元）来实现你项目的。一旦你完成了你的实现并且获得了满意的结果，请确认全面回答下面相关的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### 在这里实现你的代码\n",
    "### 必要的话你可以随意添加更多的代码单元 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 10\n",
    "_你的模型在真实数据的测试集上定位数字表现的怎么样？包含位置信息之后你的分类结果有变化吗？_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 11\n",
    "在你在**步骤3 **所捕获的图像上测试你的定位功能。模型是否准确计算出你找到的图像中的数字的边界框？如果你没有使用图形界面，您可能需要手动探索边界框。_提供一个在捕获的图像上创建边界框的示例_。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 可选步骤 5：为模型封装一个应用或者是程序\n",
    "\n",
    "为了让你的项目更进一步。如果你有兴趣，可以构建一个安卓应用程序，或者是一个更鲁棒的Python程序。这些程序能够和输入的图像交互，显示分类的数字甚至边界框。比如，你可以尝试通过将你的答案叠加在图像上像[Word Lens](https://en.wikipedia.org/wiki/Word_Lens)应用程序那样来构建一个增强现实应用程序。\n",
    "\n",
    "如何在安卓上的相机应用程序中加载TensorFlow的模型的示例代码在[TensorFlow Android demo app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android)中，你可以再这个基础上做一些简单的修改。\n",
    "\n",
    "\n",
    "如果你决定探索这条可选路径，请务必记录你的接口和实现，以及你找到的重要结果。你可以通过[点击这个链接](https://review.udacity.com/#!/rubrics/413/view)看到将被用来评价你的工作的相关条目。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可选 实现\n",
    "使用下面的代码单元（必要的话可以使用多个代码单元）来实现你项目的。一旦你完成了你的实现并且获得了满意的结果，请确认全面回答下面相关的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### 在这里实现你的代码  \n",
    "### 必要的话你可以随意添加更多的代码单元 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文档\n",
    "请提供额外的文档，这些文档要足以详细说明安卓应用程序或Python程序如何实现可视化图像中数字的分类。你的描述应该清楚描述程序或应用程序的工作原理并提供一些演示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_在这里写你的文档_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **注意:** 当你写完了所有的代码，并且回答了所有的问题。你就可以把你的 iPython Notebook 导出成 HTML 文件。你可以在菜单栏，这样导出**File -> Download as -> HTML (.html)**把这个 HTML 和这个 iPython notebook 一起做为你的作业提交。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
